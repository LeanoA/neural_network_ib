{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje supervisado en redes neuronales multicapa\n",
    "<!-- Autor -->\n",
    "- **Autor:** Alexander Leano\n",
    "- **Fecha:** 13 de Noviembre del 2024\n",
    "\n",
    "## Introducción\n",
    "En este trabajo se busca realizar un modelo de aprendizaje supervisado utilizando redes neuronales multicapas para aprender la función XOR, problema de paridad (generalización del XOR) y el mapeo logistico.\n",
    "\n",
    "El codigo fuente del trabajo se puede encontrar en el siguiente [notebook-github](https://github.com/LeanoA/neural_network_ib/blob/600066300d8e595949c79a01ca56d8521e3a1041/Practica4/Practica-4.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Función XOR\n",
    "\n",
    "La regla XOR tiene dos entradas (+1 o -1) y la salida es -1 si ambas son diferentes y +1 si ambas son iguales.  Se busca utilizar el algoritmo de retropropagacion de errores para aprender el XOR en las siguientes arquitecturas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image centered with reference and figure #-->\n",
    "<center><img src=\"Ejercicio1-Arquitectura.png\" width=\"600\" title=\"Arquitectura 1 y 2\" alt=\"Arquitectura 1\" /></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Figure 1: Dos arquitecturas posibles, izquierda Arquitectura o Red 1 derecha Arquitectura o Red 2.</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ambos casos se incluyen las unidades de entrada adicional para simular los umbrales. Se utiliza $\\text{tanh}(x)$ como función de transferencia. Se repite el proceso para 10 condiciones inciales de las conexiones.\n",
    "\n",
    "Se comparar el tiempo medio de convergencia de ambas arquitecturas.\n",
    "\n",
    "Los resultados obtenidos se representan en la *Figura 2* como el error cuadrático medio en función del número de iteraciones para ambas arquitecturas y 10 condiciones iniciales de las conexiones. Ademas se muestra el tiempo medio de convergencia para ambas arquitecturas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image centered with reference and figure #-->\n",
    "<center><img src=\"Ejercicio1-PartB-error.png\" width=\"700\" title=\"Arquitectura 1 y 2\" alt=\"Arquitectura 1\" /></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Figura 2: Error cuadrático medio en función de numero de iteración para arquitectura 1, 2 y una comparación entre los promedios promedios.</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De este grafico se puede observar que la arquitectura 2 converge más rapido que la arquitectura 1 para los 10 casos iniciales de las conexiones analizados. Se puede observar tambien que la arquitectura 1 tiene minimos locales que tardan en salir de ellos. Se puede obervar que el la arquitectura 1 aun mantiene un error cuadratico medio mayor que la arquitectura 2 despues de 2000 iteraciones producto de un minimo local que aun no convergio.\n",
    "\n",
    "Los tiempos de convergencia medios parecen ser similares, sin embargo, la arquitectura 2 muestra un mejor tiempo de convergencia para algunos casos lo que se refleja en el promedio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Función de paridad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema de paridad es una generalización del XOR para N entradas. La salida es `+1` si el producto de las `N` entradas es `+1` y `-1` si el producto de las entradas es `-1`. Se busca utilizar retropropagación para aprender el problema en la siguiente arquitectura que se muestra en la **Figura 3**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image centered with reference and figure #-->\n",
    "<center><img src=\"Ejercicio2-Arquitectura.png\" width=\"200\" title=\"Arquitectura XOR generalizado\" alt=\"Arquitectura XOR generalizado\" /></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Figure 3:</b> Arquitecturas XOR generalizado. Se resuelve para un número de entradas N=5 y un numero de neuronas ocultas de N'= 1, 3, 5, 7, 9, 11.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se buscar aprender el problema para un numero de entradas `N` fijo de 5 y un numero de neuronas `N'` en la capa oculta de 1, 3, 5, 7, 9, 11 respectivamente. De esta manera se busca analizar que sucede cuando `N'` es menor, igual o mayor a `N`. \n",
    "\n",
    "Para cada caso con diferente numero de neuronas en la capa oculta se realizó un promedio del error entre 10 procesos de entrenamiento utilizando condiciones iniciales diferentes. Los resultados obtenidos se muestran en la **Figura 4**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image centered with reference and figure #-->\n",
    "<center><img src=\"Ejercicio2-PartC.png\" width=\"700\" title=\"Arquitectura XOR generalizado\" alt=\"Arquitectura XOR generalizado\" /></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Figure 4:</b> Error cuadrático medio en función de numero de iteración para arquitectura XOR generalizado. Se resuelve para un número de entradas N=5 y un numero de neuronas ocultas de N'= 1, 3, 5, 7, 9, 11.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que cuando el numero de neuronas en la capa oculta iguala al numero de entradas la red aprende la tarea de forma satisfactoria.\n",
    "En contraste, un numero menor de neuronas en la capa oculta que neuronas de entrada, no permite aprender la tarea ya que no alcanzan los grados de libertad para ajustar la curva de separación. Vemos como en el caso de 1 y 3 neuronas en la capa oculta mantienen un error cuadratico medio alto luego de haber llegado a un mínimo despues de aproximadamente 1.000 iteraciones y el mismo se mantiene constante.\n",
    "\n",
    "Observamos, tambien, que en los casos donde el numero de neuronas en la capa oculta es mayor al número de entradas la red aprende la tarea pero esto requiere significativamente mas iteraciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mapeo logístico\n",
    "\n",
    "En esta sección se busca aprender utilizando retropropagación el mapeo logístico $x(t + 1) = 4x(t)(1 − x(t))$ en la arquitectura que se muestra en la **Figura 5**. \n",
    "\n",
    "Para ello se tiene en cuenta las siguientes consideraciones:\n",
    "- La función de activación de las neuronas en la capa oculta es $g(x) = 1/(1 + exp(−x))$.\n",
    "- Las neuronas incluyen umbrales. \n",
    "- La función de activación de la neurona de salida es lineal.\n",
    "- Se busca generar una base de datos $x(t), x(t = 1)$ iterando el mapeo y presentar 5, 10 y 100 ejemplos.\n",
    "- Se toma un 80% de los ejemplos presentados y se testea con el 20% de los ejemplos no presentados.\n",
    "\n",
    "Como objetivo, se busca comparar el error de entrenamiento con el error de generalización.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image centered with reference and figure #-->\n",
    "<center><img src=\"Ejercicio3-Arquitectura.png\" width=\"200\" title=\"Arquitectura XOR generalizado\" alt=\"Arquitectura XOR generalizado\" /></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Figure 5:</b> Arquitectura propuesta para aprender el mapeo logistico mediante retropropagación.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo se implementó utilizando la libreria de python `tensorflow` y la arquitectura del modelo se muestra en la **Figura 6**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image centered with reference and figure #-->\n",
    "<center><img src=\"Ejercicio3-model.png\" width=\"700\" title=\"Arquitectura XOR generalizado\" alt=\"Arquitectura XOR generalizado\" /></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Figure 6:</b> Arquitectura de la red propuesta utilizando la libreria de python <i>tensorflow</i>.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar el modelo se utilizó el 80% de los datos generados y se testeo con el 20% restante. Se entreno por 1500 epocas y se utilizó un tamaño de batch de 4.\n",
    "Los resultados obtenidos se muestran en la **Figura 7**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image centered with reference and figure #-->\n",
    "<center><img src=\"Ejercicio3-error.png\" width=\"700\" title=\"Logistic Regresion MSE \" alt=\"Logistic Regresion MSE\" /></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Figure 7:</b> Error cuadrático medio en función del numero de iteraciones para 5, 10 y 100 ejemplos utilizando el 80% para entrenamiento y el 20% para testeo.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la **Figura 7** se puede observar que en todos los casos pareciera aprender. Tambien vemos que el error disminuye rapidamente en las primeras iteraciones y luego se mantiene practicamente constante. El caso de 10 y 100 ejemplos tienen un comportamiento similar pero el error es menor en el caso de 100 ejemplos. Con respecto al error de generalización el error es menor para el caso de 100 ejemplos indicando que el modelo generaliza mejor con mas datos. \n",
    "El caso de 5 ejemplos es un caso especial ya que debido a la poca cantidad de datos el error de generalización es mayor que el error de entrenamiento.\n",
    "\n",
    "Una vez entrenado el modelo se puede comparar los valores predichos con los valores reales. En la **Figura 8** se muestra la comparación entre los valores reales y los valores predichos para los distintos casos. Se muestra ademas, los datos de entrenamiento y testeo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image centered with reference and figure #-->\n",
    "<center><img src=\"Ejercicio3.png\" width=\"700\" title=\"Prediccion de la regresion logistica\" alt=\"Prediccion de la regresion logistica\" /></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Figure 8:</b> Comparación de la prediccion de la regresión logistica para los distintos casos de 5, 10 y 100 ejemplos con la curva real del mapeo logistico.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al comparar los distintos modelos con la curva real del mapeo logistico vemos que el error tiende a cero por que se logra ajustar la curva con los ejemplos de entrenamiento. Sin embargo al generalizar el error aumenta. El error de generalización disminuye a medida que aumenta el numero de ejemplos utilizados para entrenar el modelo. Como caso extreme vemos el modelo para 5 ejemplos que al utilizar el 80% para entrenamiento significo solo utilizar 4 ejemples para entrenar. Dependiendo de que tan distribuidos esten los ejemplos en el espacio de entrada el modelo puede generalizar mejor o peor. Al tener mas ejemplos, hay una mayor probabilidad de que se hayen mejor distribuidos y el modelo generalice mejor. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
