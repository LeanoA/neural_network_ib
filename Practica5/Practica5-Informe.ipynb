{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales - Aprendizaje no supervisado\n",
    "\n",
    "Autor: Alexander Leaño\n",
    "Fecha: 29 de Noviembre de 2024\n",
    "\n",
    "## Introducción\n",
    "\n",
    "El aprendizaje no supervisado es una rama del aprendizaje automático que se encarga de encontrar patrones en los datos sin la necesidad de tener etiquetas. En este trabajo se busca analizar el comportamiento de una red neuronal lineal de una capa y también red neuronal de Kohonen. \n",
    "\n",
    "El codigo fuente del trabajo se puede encontrar en el siguiente [notebook-github](https://github.com/LeanoA/neural_network_ib/blob/096a2bcd94735913af40f4262d72f013605cb86d/Practica5/Practice5.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema 1: Red Neuronal Lineal de una capa\n",
    "Se busca realizar el analisis de una red neuronal lineal con una sola capa de neuronas. La red neuronal tiene 4 entradas y una salida. La ecuación de la salida es:\n",
    "\n",
    "$$\n",
    "V = \\sum_{j=1}^4 w_j \\xi_j \\tag{1}\n",
    "$$\n",
    "\n",
    "La distribución de probabilidad de las entradas es una distribución Gaussiana con matriz de correlación $\\Sigma$:\n",
    "\n",
    "$$\n",
    "P(\\bar{\\xi}) = \\frac{1}{(2\\pi)^2 \\sqrt{\\det(\\Sigma)}} \n",
    "\\exp\\left(-\\frac{1}{2} \\bar{\\xi}^T \\Sigma^{-1} \\bar{\\xi}\\right) \\tag{2}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "$$\n",
    "\\Sigma = \n",
    "\\begin{pmatrix}\n",
    "2 & 1 & 1 & 1 \\\\\n",
    "1 & 2 & 1 & 1 \\\\\n",
    "1 & 1 & 2 & 1 \\\\\n",
    "1 & 1 & 1 & 2 \\\\\n",
    "\\end{pmatrix} \\tag{3}\n",
    "$$\n",
    "\n",
    "\n",
    "Para esto, se parte de pesos $w_j$ aleatorios y pequeños para aplicar la regla de aprendizaje:\n",
    "\n",
    "$$\n",
    "\\Delta w_j = \\eta V (\\xi_j - V w_j) \\tag{4}\n",
    "$$\n",
    "\n",
    "Se busca comparar los valores asintóticos de los pesos con los autovectores de la matriz $\\Sigma$. Para esto se busca hallar el autovector asociado al mayor de los autovales de la matriz de covarianza $\\Sigma$ para comparar con los pesos de la red. El autovector asociado al mayor autovector es:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "0.5 \\\\\n",
    "0.5 \\\\\n",
    "0.5 \\\\\n",
    "0.5 \\\\\n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "\n",
    "*Nota*: Como una ayuda para el cálculo va a  ser útil utilizar la raíz cuadrada de la matriz de correlación:\n",
    "\n",
    "$$\n",
    "\\Sigma^{1/2} = \n",
    "\\begin{pmatrix}\n",
    "1.309 & 0.309 & 0.309 & 0.309 \\\\\n",
    "0.309 & 1.309 & 0.309 & 0.309 \\\\\n",
    "0.309 & 0.309 & 1.309 & 0.309 \\\\\n",
    "0.309 & 0.309 & 0.309 & 1.309 \\\\\n",
    "\\end{pmatrix} \\tag{5}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "En la Figura 1 se muestra la evolución del error cuadrático medio entre los pesos $w_j$ y los componentes del mayor autovector de la matriz de correlación $\\Sigma$ en función de las iteraciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Problema1.png\" width=\"600\" title=\" Error cuadrático medio entre el valor de los pesos de la red y el autovector asociado al autovalor más grande de la matriz de covariza $\\Sigma$.\" alt=\"Error cuadrático medio entre el valor de los pesos de la red y el autovector asociado al autovalor más grande de la matriz de covariza $\\Sigma$.\" /></center>\n",
    "\n",
    "<p style=\"text-align: center\">\n",
    "    <b>\n",
    "    Figure 1:</b> Error cuadrático medio entre el valor de los pesos de la red y el autovector asociado al autovalor más grande de la matriz de covarianza.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizando este resultado, se puede observar como el error disminuye rápidamente en las primeras iteraciones y luego se estabiliza en un valor cercano a cero. Esto indica que los pesos de la red convergen al autovector asociado al autovalor más grande de la matriz de covarianza $\\Sigma$. \n",
    "\n",
    "De esta manera podemos concluir que al utilizar este algoritmo de aprendizaje, de una sola salida, realizamos una reducción dimensional de los datos, ya que los pesos de la red se ajustan para que la salida de la red sea proporcional al autovector asociado al mayor autovalor de la matriz de covarianza $\\Sigma$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema 2: Red Neuronal de Kohonen con Entradas en Coordenadas Polares\n",
    "\n",
    "Este problema investiga un mapa autoorganizado de Kohonen (SOM: *Self Organization Map*) con las siguientes especificaciones:\n",
    "- **Neuronas de Entrada**: 2 (correspondientes a $ \\xi_1 $ y $ \\xi_2 $).\n",
    "- **Neuronas de Salida**: 10, dispuestas linealmente.\n",
    "- **Distribución de Entrada**:\n",
    "  - $ P(r, \\theta) = \\text{constante} $ si $ r \\in [0.9, 1.1] $ y $ \\theta \\in [0, \\pi] $,\n",
    "  - $ P(r, \\theta) = 0 $ en caso contrario.\n",
    "  - $ r $ y $ \\theta $ son coordenadas polares derivadas como:\n",
    "    - $ r = \\sqrt{\\xi_1^2 + \\xi_2^2} $,\n",
    "    - $ \\theta = \\arctan(\\xi_2 / \\xi_1) $.\n",
    "- **Función de Vecindad**: Gaussiana:\n",
    "  $$\n",
    "  \\Lambda(i, i^*) \\propto \\exp\\left(-\\frac{(i - i^*)^2}{2\\sigma^2}\\right)\n",
    "  $$\n",
    "\n",
    "### Metodología\n",
    "El SOM fue entrenado con:\n",
    "- **Número de Neuronas**: $10$,\n",
    "- **Tasa de Aprendizaje**: $ \\eta = 0.01 $,\n",
    "- **Ancho de Vecindad ($ \\sigma $)**: $ \\sigma = \\{2, 1, 0.5, 0.1\\} $,\n",
    "- **Número de Iteraciones**: $ 10,000 $.\n",
    "- **Pesos Iniciales**: $\\xi_1$ se inicilizó con valores entre -1 y 1 equidistantes para completar el numero de neuronas. Mientras que $\\xi_2$ se inicilizó con valores en 0.\n",
    "\n",
    "#### Pasos de Simulación\n",
    "1. **Muestreo de Entradas**:\n",
    "   - Se generaron muestras aleatorias $ \\bar{\\xi} $ de la distribución definida $ P(r, \\theta) $.\n",
    "2. **Actualización de Pesos**:\n",
    "   - Se actualizaron los pesos sinápticos de todas la $ \\mathbf{w} $ utilizando la regla de aprendizaje de Kohonen:\n",
    "     $$\n",
    "     \\Delta w_i = \\eta \\cdot \\Lambda(i, i^*) \\cdot (\\xi - w_i),\n",
    "     $$\n",
    "     donde $ i^* $ es el índice de la neurona ganadora. La neurona ganadora se determinón como la de menor distancia en norma 2 a la entrada.\n",
    "3. **Función de Vecindad**:\n",
    "   - Se ajustó el ancho gaussiano $ \\sigma $ para analizar su impacto en la convergencia de los pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "\n",
    "Los resultados para los distintos valores de $ \\sigma $ se presentan en las figuras 2, 3, 4 y 5.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Problema2-sigma2.png\" width=\"600\" title=\"Aprendizaje con Red de Kohonen para coordenas polares con valor de \\sigma = 2\" alt=\"Aprendizaje con Red de Kohonen para coordenas polares con valor de \\sigma = 2\" /></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>\n",
    "    Figure 2:</b> Aprendizaje con Red de Kohonen para coordenas polares con valor de sigma 2.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Problema2-sigma1.png\" width=\"600\" title=\"Aprendizaje con Red de Kohonen para coordenas polares con valor de \\sigma = 1\" alt=\"Aprendizaje con Red de Kohonen para coordenas polares con valor de \\sigma = 1\" />\n",
    "<p style=\"text-align: center\">\n",
    "    <b>\n",
    "    Figure 3:</b> Aprendizaje con Red de Kohonen para coordenas polares con valor de sigma 1.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Problema2-sigma0.5.png\" width=\"600\" title=\"Aprendizaje con Red de Kohonen para coordenas polares con valor de \\sigma = 0.1\" alt=\"Aprendizaje con Red de Kohonen para coordenas polares con valor de \\sigma = 0.1\" /></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>\n",
    "    Figure 4:</b> Aprendizaje con Red de Kohonen para coordenas polares con valor de sigma 0.5.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Problema2-sigma0.1.png\" width=\"600\" title=\"Aprendizaje con Red de Kohonen para coordenas polares con valor de \\sigma = 0.1\" alt=\"Aprendizaje con Red de Kohonen para coordenas polares con valor de \\sigma = 0.1\" /></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>\n",
    "    Figure 5:</b> Aprendizaje con Red de Kohonen para coordenas polares con valor de sigma 0.1.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De los resultados obtenidos se puede observar que para $\\sigma$ 0.5 se obtiene suficiente flexibilidad para que los pesos de la red se ajusten a la distribución de probabilidad de las entradas. Sin embargo, al aumentar o disminuir el valor de $\\sigma$ se observa que la red no logra converger a la distribución de probabilidad de las entradas. Para $\\sigma$ 1 y 2 observamos que la influencia de la neurona ganadora en la actualización de los pesos es muy grande, lo que impide que la red se ajuste a la distribución de probabilidad de las entradas. Por otro lado para $\\sigma$ 0.1 tampoco se logra ajustar la red a la distribución de probabilidad de las entradas, pero en este caso la influencia de la neurona ganadora es muy pequeña. Para valor pequeños algunos pesos quedan sin actualizar ya que pocas veces fueron la neurona ganadora.\n",
    "\n",
    "En conclución, el valor de $\\sigma$ es un hiperparámetro importante en el entrenamiento de una red de Kohonen, ya que determina la influencia de la neurona ganadora en la actualización de los pesos. Un valor adecuado de $\\sigma$ permite que la red se ajuste a la distribución de probabilidad de las entradas. Valores muy grandes no brindan la suficiente flexibilidad para que los pesos de la red se ajusten a la distribución de probabilidad de las entradas. Por otro lado, valores muy pequeños no permite la actualización de todos los pesos de la red, ya que pocas veces son la neurona ganadora y la actualización indirecta es debil por la función de vecindad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
